---
title: open projects
description: coming up with a new thing to work on, but now i got two
images:
---
been poking around w/ some stuff

for work, i gotta figure out lmsys-style ELO arena + knowledge graphs (probably via neo4j)

looking at various libs + tools for personal projects, going to figure out how i'll split time between RPi security camera and character/rp models w/ memory

interface: probably discord bot, maybe will do some svelte stuff to spin up the data exploration + eval side

inference probably llama.cpp + server... vLLM is missing support for GGUFs and other quantized formats. saw something about quantization being bad on llama3 (perplexity wise), but maybe for character models this is actually a good thing

retrieval... not sure lol
- Vespa
- cohere embeddings
- colbert
- duckdb
- maybe knowlege graphs are necessary here? neo4j

finally got around to learning some more docker stuff... it ain't pretty

---

bought NoIR raspberry pi cameras... with no IR lights - gonna figure that out separately

---

interesting finding w/ vLLM + P2P patch

```
WARNING 04-22 16:58:41 custom_all_reduce.py:52] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
```

installed the lib locally and stopped vllm from preventing this and it seems to work, but can't really see it doing anything good for inference speed
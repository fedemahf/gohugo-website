---
title: snippets
description: |
    immortalizing some fun stuff that i come across as i
    explore the internet
draft: false
images:
- /apple-touch-icon.png
---

[github project research list](https://github.com/stars/hitorilabs/lists/research)

LeakingAlpha α

4. [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html) - twitter [@cHHillee](https://twitter.com/cHHillee)
3. [Torch Dispatch + FLOP counter](https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557) - twitter [@cHHillee](https://twitter.com/cHHillee)
2. [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301) - twitter [@_akhaliq](https://twitter.com/_akhaliq)
1. [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models](https://arxiv.org/abs/2110.07178) - twitter [@yacineMTB](https://twitter.com/yacineMTB)


classics
1. [Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
2. [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
3. [Sentence Embeddings using Siamese BERT-Networks ](https://arxiv.org/abs/1908.10084)

4. [Attention is All You Need" (2017)](https://arxiv.org/abs/1706.03762) (Google Brain)
5. [ImageNet Classification with Deep Convolutional Neural Networks" (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (University of Toronto)
6. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](https://arxiv.org/abs/1810.04805)(Google)
7. [RoBERTa: A Robustly Optimized BERT Pretraining Approach" (2019)](https://arxiv.org/abs/1907.11692) (Facebook AI)
8. [ELMo: Deep contextualized word representations" (2018)]( https://arxiv.org/abs/1802.05365) (Allen Institute for Artificial Intelligence)
9. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)](https://arxiv.org/abs/1901.02860) (Google Al Language)
10. ["XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)](https://arxiv.org/abs/1906.08237)(Google AI Language)
11. [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2020)](https://arxiv.org/abs/1910.10683) (Google Research)
12. [Language Models are Few-Shot Learners (2021)](https://arxiv.org/abs/2005.14165)

13. [Dynamic Routing Between Capsules (2017)](https://arxiv.org/abs/1710.09829) (Google Brain)
14. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2016)](https://arxiv.org/abs/1511.06434) (University of Montreal)
15. [Generative Adversarial Networks (2014)](https://arxiv.org/abs/1406.2661) (University of Montreal)

technical
1. [Asianometry Semiconductor Series](https://www.youtube.com/watch?v=Pt9NEnWmyMo&list=PLKtxx9TnH76QY5FjmO3NaUkVJvTPN9Vmg)
2. [Deep Neural Networks for YouTube Recommendations](https://dl.acm.org/doi/pdf/10.1145/2959100.2959190)
3. [Sentence-BERT (SBERT)](https://arxiv.org/pdf/1908.10084.pdf)
4. [MiniLM](https://arxiv.org/pdf/2002.10957.pdf)
5. [Dropout](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
6. [Wide & Deep Learning for Recommender Systems - Google](https://arxiv.org/pdf/1606.07792.pdf)
7. [Kraken: Memory-Efficient Continual Learning](http://storage.cs.tsinghua.edu.cn/papers/sc20-kraken.pdf/)
8. [Rijndael](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard) (AES)
9. [Monolith - Tiktok](https://arxiv.org/pdf/2209.07663.pdf)

anime
1. [Re:Zero − Starting Life in Another World](https://en.wikipedia.org/wiki/Re:Zero_%E2%88%92_Starting_Life_in_Another_World)
2. [Cyberpunk Edgerunners](https://en.wikipedia.org/wiki/Cyberpunk:_Edgerunners)
3. [Bocchi the Rock!](https://en.wikipedia.org/wiki/Bocchi_the_Rock!)
4. [Mushoku Tensei: Jobless Reincarnation](https://en.wikipedia.org/wiki/Mushoku_Tensei)
5. [The Eminence in Shadow](https://en.wikipedia.org/wiki/The_Eminence_in_Shadow)
6. [Bleach](https://en.wikipedia.org/wiki/Bleach_(TV_series))
7. [Attack on Titan](https://en.wikipedia.org/wiki/Attack_on_Titan)
8. [Your Name](https://en.wikipedia.org/wiki/Your_Name)
9. [Your Lie in April](https://en.wikipedia.org/wiki/Your_Lie_in_April)
